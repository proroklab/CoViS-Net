<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
    <meta property="og:url" content="URL OF THE WEBSITE"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>


    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Academic Project Page</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            }
        };
    </script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">CoViS-Net: A Cooperative Visual Spatial Foundation Model
                        for Multi-Robot Applications</h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=GvdaoD4AAAAJ&hl=en" target="_blank">Jan Blumenkamp</a>,
                        </span>
                        <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=KvCgriAAAAAJ&hl=en" target="_blank">Steven Morad</a>,
                        </span>
                      <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=jw0Uz9wAAAAJ&hl=en" target="_blank">Jennifer Gielis</a> and
                        </span>
                      <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=o7xMDgEAAAAJ&hl=en" target="_blank">Amanda Prorok</a>
                        </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block">Department of Computer Science and Technology, University of Cambridge<br>CoRL 2024</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- Arxiv PDF link -->
                            <span class="link-block">
                                <a href="https://arxiv.org/pdf/2405.01107" target="_blank"
                                   class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fas fa-file-pdf"></i>
                                    </span>
                                    <span>Paper</span>
                                </a>
                            </span>

                            <!-- Github link -->
                            <span class="link-block">
                                <a href="https://github.com/proroklab/CoViS-Net" target="_blank"
                                   class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                      <i class="fab fa-github"></i>
                                    </span>
                                    <span>Code</span>
                                </a>
                            </span>

                            <!-- YouTube link -->
                            <span class="link-block">
                                <a href="https://drive.google.com/file/d/1U8lX_DXu1JiwVvmlPRgOgqU6J5WTEc6p/preview" target="_blank"
                                   class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fas fa-file-pdf"></i>
                                    </span>
                                    <span>Video</span>
                                </a>
                            </span>

                            <!-- ArXiv abstract Link -->
                            <span class="link-block">
                                <a href="https://arxiv.org/abs/2405.01107" target="_blank"
                                class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>arXiv</span>
                                </a>
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
    <div class="container">
        <div class="row mb-4">
            <div class="col-lg-7 mx-auto">
                <video class="shadow-md rounded" poster="" id="tree" autoplay controls muted loop height="100%">
                    <source src="static/videos/hero.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered mt-1 mb-3">
                    <p class="mb-2">
                        Our model can be used to control the relative pose of multiple follower robots (depicted in yellow and magenta) to a leader robot (blue) following a reference trajectory using visual cues in the environment only (the field-of-view is depicted by cones).
                    </p>
                </h2>
            </div>
        </div>
    </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Autonomous robot operation in unstructured environments is often underpinned by spatial
                        understanding through vision.
                        Systems composed of multiple concurrently operating robots additionally require access to
                        frequent, accurate and reliable pose estimates.
                        In this work, we propose CoViS-Net, a decentralized visual spatial foundation model that learns
                        spatial priors from data, enabling pose estimation as well as spatial comprehension.
                        Our model is fully decentralized, platform-agnostic, executable in real-time using onboard
                        compute, and does not require existing networking infrastructure.
                        CoViS-Net provides relative pose estimates and a local bird's-eye-view (BEV) representation,
                        even without camera overlap between robots (in contrast to classical methods), and can predict
                        BEV representations of unseen regions.
                        We demonstrate its use in a multi-robot formation control task across various real-world
                        settings.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->

<!-- Youtube video -->
<section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container">
            <!-- Paper video. -->
            <h2 class="title is-3 has-text-centered">Video Presentation</h2>
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">

                    <div class="publication-video">
                        <!-- Youtube embed code here -->
                        <iframe src="https://drive.google.com/file/d/1U8lX_DXu1JiwVvmlPRgOgqU6J5WTEc6p/preview" frameborder="0"
                                allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End youtube video -->

<!-- Supplementary -->
<section class="section hero">
    <div class="container">
        <div class="row is-centered has-text-centered">
            <h3 class=" is-3 pb-2">Indoor Experiments</h3>
            <div class="col-lg-6">
                <div class="publication-video">
                    <iframe src="https://drive.google.com/file/d/1_Rm6ClbvkfZvi_Kcg2rPAvqg06iECJK_/preview" frameborder="0"
                            allow="autoplay; encrypted-media" allowfullscreen></iframe>
                </div>
            </div>
            <div class="col-lg-6">
                <div class="content has-text-justified">
                    <p class="pt-2 ps-lg-4">
                        We extensively tested CoViS-Net in various indoor environments, including corridors, offices, and rooms with challenging lighting.
                        Using up to four follower robots maintaining fixed relative poses to a remote-controlled leader, we evaluated performance across different scenarios.
                        Results showed consistent accuracy, with median localization errors as low as 22 cm and 5.2° for visible samples.
                        Even without direct visual overlap, the system achieved median errors of 77-146 cm for position estimates.
                        These tests demonstrated CoViS-Net's robustness to real-world variability in lighting, layout, and obstacles.
                    </p>
                </div>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="row is-centered has-text-centered">
            <h3 class=" is-3 pb-2">Outdoor Experiments</h3>
            <div class="col-lg-6">
                <div class="publication-video">
                    <iframe src="https://drive.google.com/file/d/1cR45bfvyJ4SloeMLeY-ni5u64e8whVRd/preview" frameborder="0"
                            allow="autoplay; encrypted-media" allowfullscreen></iframe>
                </div>
            </div>
            <div class="col-lg-6">
                <div class="content has-text-justified">
                    <p class="pt-2 ps-lg-4">
                        Despite being trained solely on indoor data, CoViS-Net showed impressive generalization to outdoor settings.
                        Experiments in streets and open areas yielded median pose estimation errors of 134 cm and 9.5° for non-overlapping views, and 49 cm and 6.9° for overlapping views.
                        While slightly less accurate than indoor performance, these results remain valuable for many multi-robot applications.
                        The system's adaptability to outdoor environments is attributed to the robust spatial understanding of the DinoV2 encoder, opening possibilities for urban exploration, search and rescue, and autonomous navigation in unstructured outdoor spaces.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End Supplementary -->

<!--BEV snippet carousel -->
<section class="hero is-light is-small">
    <div class="hero-body">
        <div class="container">
            <div class="row is-centered has-text-centered">
                <h1 class="title is-3">Real-World Dataset</h1>
            </div>
            <div class="row is-centered has-text-centered">
                <div class="content col-lg-6 offset-lg-3 pt-3">
                    <p>We show renderings on all real-world dataset samples.</p>
                </div>
            </div>
        </div>
        <div class="container">
            <div id="bev-carousel" class="carousel results-carousel">
                <div class="item item-sn05-01">
                    <h3 class="has-text-centered">Office A</h3>
                    <video poster="" id="item-sn05-01" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/sn05_01.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-sn05-02">
                    <h3 class="has-text-centered">Office B</h3>
                    <video poster="" id="item-sn05-02" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/sn05_02.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-corridor-01">
                    <h3 class="has-text-centered">Corridor A</h3>
                    <video poster="" id="item-corridor-01" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/sn-corridor_01.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-sn-balcony-01">
                    <h3 class="has-text-centered">Sunny</h3>
                    <video poster="" id="item-sn-balcony-01" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/sn-balcony_01.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-s-street-01">
                    <h3 class="has-text-centered">Corridor B</h3>
                    <video poster="" id="item-s-street-01" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/s-street_01.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-intellab-01">
                    <h3 class="has-text-centered">Study A</h3>
                    <video poster="" id="item-intellab-01" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/intellab_01.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-intellab-02">
                    <h3 class="has-text-centered">Study B</h3>
                    <video poster="" id="item-intellab-02" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/intellab_02.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-intellab-terrace-00">
                    <h3 class="has-text-centered">Outdoor</h3>
                    <video poster="" id="item-intellab-terrace-00" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/intellab_terrace_00.mp4"
                                type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>
</section>
<!--End BEV snippet carousel -->

<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container">
            <h2 class="title">Poster</h2>

            <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
            </iframe>

        </div>
    </div>
</section> -->
<!--End paper poster -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title has-text-centered">BibTeX</h2>
        <pre><code>
            @inproceedings{blumenkamp2024covisnet,
              title={CoViS-Net: A Cooperative Visual Spatial Foundation Model for Multi-Robot Applications},
              author={Blumenkamp, Jan and Morad, Steven and Gielis, Jennifer and Prorok, Amanda},
              booktitle={Conference on Robot Learning},
              year={2024}
            }
        </code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">

                    <p>
                        This page was built using the <a
                            href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic
                        Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io"
                                                                                target="_blank">Nerfies</a> project
                        page.
                    </p>

                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
