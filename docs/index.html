<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="CoViS-Net: A decentralized visual spatial foundation model for multi-robot applications, enabling pose estimation and spatial comprehension for multi-robot systems.">

    <meta property="og:title" content="CoViS-Net: Cooperative Visual Spatial Foundation Model"/>
    <meta property="og:description" content="A novel cooperative perception model for multi-robot systems providing real-time, decentralized pose estimation and BEV representation prediction using only monocular cameras."/>
    <meta property="og:url" content="https://proroklab.github.io/CoViS-Net/"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
    <meta property="og:image" content="docs/static/images/hero.jpg" />
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="382"/>

    <meta name="twitter:title" content="CoViS-Net: Multi-Robot Visual Spatial Foundation Model">
    <meta name="twitter:description" content="Decentralized, real-time pose estimation and spatial understanding for robot swarms, demonstrated in various real-world settings.">
    <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
    <meta name="twitter:image" content="docs/static/images/hero.jpg">
    <meta name="twitter:card" content="summary_large_image">

    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="Multi-Robot Systems, Robot Perception, Foundation Models, Visual Spatial Understanding, Decentralized Pose Estimation, Bird's-Eye-View Representation, Uncertainty-Aware Robotics">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Academic Project Page</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            }
        };
    </script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">CoViS-Net: A Cooperative Visual Spatial Foundation Model
                        for Multi-Robot Applications</h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=GvdaoD4AAAAJ&hl=en" target="_blank">Jan Blumenkamp</a>,
                        </span>
                        <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=KvCgriAAAAAJ&hl=en" target="_blank">Steven Morad</a>,
                        </span>
                      <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=jw0Uz9wAAAAJ&hl=en" target="_blank">Jennifer Gielis</a> and
                        </span>
                      <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=o7xMDgEAAAAJ&hl=en" target="_blank">Amanda Prorok</a>
                        </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block">Department of Computer Science and Technology, University of Cambridge<br>CoRL 2024</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- Arxiv PDF link -->
                            <span class="link-block">
                                <a href="https://arxiv.org/pdf/2405.01107" target="_blank"
                                   class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fas fa-file-pdf"></i>
                                    </span>
                                    <span>Paper</span>
                                </a>
                            </span>

                            <!-- Github link -->
                            <span class="link-block">
                                <a href="https://github.com/proroklab/CoViS-Net" target="_blank"
                                   class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                      <i class="fab fa-github"></i>
                                    </span>
                                    <span>Code</span>
                                </a>
                            </span>

                            <!-- YouTube link -->
                            <span class="link-block">
                                <a href="https://drive.google.com/file/d/1U8lX_DXu1JiwVvmlPRgOgqU6J5WTEc6p/preview" target="_blank"
                                   class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="fas fa-file-pdf"></i>
                                    </span>
                                    <span>Video</span>
                                </a>
                            </span>

                            <!-- ArXiv abstract Link -->
                            <span class="link-block">
                                <a href="https://arxiv.org/abs/2405.01107" target="_blank"
                                class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>arXiv</span>
                                </a>
                            </span>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row mb-4">
                <div class="col-lg-7 mx-auto has-text-centered">
                    <p class="mb-2">
                        <b>TL;DR:</b> CoViS-Net is a decentralized, real-time, multi-robot visual spatial model that learns spatial priors from data to provide relative pose estimates and bird's-eye-view representations.
                        We demonstrate its effectiveness in real-world multi-robot formation control tasks.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
    <div class="container">
        <div class="row mb-4">
            <div class="col-lg-7 mx-auto">
                <video class="shadow-md rounded" poster="" id="tree" autoplay controls muted loop height="100%">
                    <source src="static/videos/hero.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered mt-1 mb-3">
                    <p class="mb-2">
                        Our model can be used to control the relative pose of multiple follower robots (depicted in yellow and magenta) to a leader robot (blue) following a reference trajectory using visual cues in the environment only (the field-of-view is depicted by cones).
                    </p>
                </h2>
            </div>
        </div>
    </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Autonomous robot operation in unstructured environments is often underpinned by spatial
                        understanding through vision.
                        Systems composed of multiple concurrently operating robots additionally require access to
                        frequent, accurate and reliable pose estimates.
                        In this work, we propose CoViS-Net, a decentralized visual spatial foundation model that learns
                        spatial priors from data, enabling pose estimation as well as spatial comprehension.
                        Our model is fully decentralized, platform-agnostic, executable in real-time using onboard
                        compute, and does not require existing networking infrastructure.
                        CoViS-Net provides relative pose estimates and a local bird's-eye-view (BEV) representation,
                        even without camera overlap between robots (in contrast to classical methods), and can predict
                        BEV representations of unseen regions.
                        We demonstrate its use in a multi-robot formation control task across various real-world
                        settings.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->

<!-- Method -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="row is-centered has-text-centered">
            <div class="col-lg-6">
                <img src="static/images/model.jpg" alt="Model architecture" class="center-image"/>
            </div>
            <div class="col-lg-6">
                <h3 class=" is-3 pb-2">Model</h3>
                <div class="content has-text-justified">
                    <p class="pt-2 ps-lg-4">
                        CoViS-Net consists of four primary components: an image encoder $f_\mathrm{enc}$, a pairwise pose encoder $f_\mathrm{pose}$, a multi-node aggregator $f_\mathrm{agg}$, and a BEV predictor $f_\mathrm{BEV}$.
                        The image encoder uses a pre-trained DinoV2 model with additional layers to generate the embedding $\mathbf{E}_i$ from image $I_i$.
                        These embeddings are communicated between robots. The pose estimator takes two embeddings $\mathbf{E}_i$ and $\mathbf{E}_j$ as input and predicts pose estimates with uncertainty.
                        The multi-node aggregator combines the estimated poses with image embeddings from multiple robots and aggregates them into a common representation.
                        Finally, the BEV predictor generates a bird's-eye-view representation from the aggregated information.
                    </p>
                </div>
            </div>
        </div>
        <div class="row is-centered has-text-centered">
            <div class="col-lg-6">
                <h3 class=" is-3 pb-2">Training</h3>
                <div class="content has-text-justified">
                    <p class="pt-2 ps-lg-4">
                        We train CoViS-Net using supervised learning on data from the Habitat simulator with the HM3D dataset.
                        This provides a diverse range of photorealistic indoor environments.
                        Our loss functions include components for pose estimation, uncertainty prediction, and BEV representation accuracy.
                        CoViS-Net incorporates uncertainty estimation using Gaussian Negative Log Likelihood (GNLL) Loss.
                        This allows the model to learn and predict aleatoric uncertainty $\hat{\sigma}^2$ from data points $\mu$, which is crucial for downstream robotic applications.
                        By providing uncertainty estimates, the system can make more informed decisions in challenging scenarios.
                    </p>
                </div>
            </div>
            <div class="col-lg-6">
                <img src="static/images/gnll.png" alt="Model architecture" class="center-image"/>
            </div>
        </div>
    </div>
</section>
<!-- End Method -->

<!-- Youtube video -->
<section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container">
            <!-- Paper video. -->
            <h2 class="title is-3 has-text-centered">Video Presentation</h2>
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">

                    <div class="publication-video">
                        <!-- Youtube embed code here -->
                        <iframe src="https://drive.google.com/file/d/1U8lX_DXu1JiwVvmlPRgOgqU6J5WTEc6p/preview" frameborder="0"
                                allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End youtube video -->

<!-- Supplementary -->
<section class="section hero">
    <div class="container">

        <div class="row is-centered has-text-centered">
            <div class="col-lg-6">
                <div class="publication-video">
                    <iframe src="https://drive.google.com/file/d/1_zwDaEHEb6hpuCL1mO5TiBMh5gKYOYJG/preview" frameborder="0"
                            allow="autoplay; encrypted-media" allowfullscreen></iframe>
                </div>
            </div>
            <div class="col-lg-6">
                <h3 class=" is-3 pb-2">Heterogeneous deployment</h3>
                <div class="content has-text-justified">
                    <p class="pt-2 ps-lg-4">
                        To demonstrate CoViS-Net's flexibility and platform-agnostic nature, we conducted heterogeneous multi-robot experiments.
                        The system was deployed on a diverse team of robots, including wheeled platforms and a quadruped (Unitree Go1).
                        Despite the significant differences in robot morphology, dynamics, and camera placement, CoViS-Net maintained consistent performance across all platforms.
                        This experiment showcased the model's ability to generalize across different robot types without requiring platform-specific training or adjustments, highlighting its potential for diverse multi-robot applications and easy integration into existing robotic systems.
                    </p>
                </div>
            </div>
        </div>
        <div class="row is-centered has-text-centered">
            <div class="col-lg-6">
                <h3 class=" is-3 pb-2">Indoor Experiments</h3>
                <div class="content has-text-justified">
                    <p class="pt-2 ps-lg-4">
                        We extensively tested CoViS-Net in various indoor environments, including corridors, offices, and rooms with challenging lighting.
                        Using up to four follower robots maintaining fixed relative poses to a remote-controlled leader, we evaluated performance across different scenarios.
                        Results showed consistent accuracy, with median localization errors as low as 22 cm and 5.2° for visible samples.
                        Even without direct visual overlap, the system achieved median errors of 77-146 cm for position estimates.
                        These tests demonstrated CoViS-Net's robustness to real-world variability in lighting, layout, and obstacles.
                    </p>
                </div>
            </div>
            <div class="col-lg-6">
                <div class="publication-video">
                    <iframe src="https://drive.google.com/file/d/1_Rm6ClbvkfZvi_Kcg2rPAvqg06iECJK_/preview" frameborder="0"
                            allow="autoplay; encrypted-media" allowfullscreen></iframe>
                </div>
            </div>
        </div>
        <div class="row is-centered has-text-centered">
            <div class="col-lg-6">
                <div class="publication-video">
                    <iframe src="https://drive.google.com/file/d/1cR45bfvyJ4SloeMLeY-ni5u64e8whVRd/preview" frameborder="0"
                            allow="autoplay; encrypted-media" allowfullscreen></iframe>
                </div>
            </div>
            <div class="col-lg-6">
                <h3 class=" is-3 pb-2">Outdoor Experiments</h3>
                <div class="content has-text-justified">
                    <p class="pt-2 ps-lg-4">
                        Despite being trained solely on indoor data, CoViS-Net showed impressive generalization to outdoor settings.
                        Experiments in streets and open areas yielded median pose estimation errors of 134 cm and 9.5° for non-overlapping views, and 49 cm and 6.9° for overlapping views.
                        While slightly less accurate than indoor performance, these results remain valuable for many multi-robot applications.
                        The system's adaptability to outdoor environments is attributed to the robust spatial understanding of the DinoV2 encoder, opening possibilities for urban exploration, search and rescue, and autonomous navigation in unstructured outdoor spaces.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End Supplementary -->

<!--BEV snippet carousel -->
<section class="hero is-light is-small">
    <div class="hero-body">
        <div class="container">
            <div class="row is-centered has-text-centered">
                <h1 class="title is-3">Real-World Dataset</h1>
            </div>
            <div class="row is-centered has-text-centered">
                <div class="content col-lg-6 offset-lg-3 pt-3">
                    <p>We show renderings on all real-world dataset samples.</p>
                </div>
            </div>
        </div>
        <div class="container">
            <div id="bev-carousel" class="carousel results-carousel">
                <div class="item item-sn05-01">
                    <h3 class="has-text-centered">Office A</h3>
                    <video poster="" id="item-sn05-01" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/sn05_01.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-sn05-02">
                    <h3 class="has-text-centered">Office B</h3>
                    <video poster="" id="item-sn05-02" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/sn05_02.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-corridor-01">
                    <h3 class="has-text-centered">Corridor A</h3>
                    <video poster="" id="item-corridor-01" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/sn-corridor_01.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-sn-balcony-01">
                    <h3 class="has-text-centered">Sunny</h3>
                    <video poster="" id="item-sn-balcony-01" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/sn-balcony_01.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-s-street-01">
                    <h3 class="has-text-centered">Corridor B</h3>
                    <video poster="" id="item-s-street-01" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/s-street_01.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-intellab-01">
                    <h3 class="has-text-centered">Study A</h3>
                    <video poster="" id="item-intellab-01" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/intellab_01.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-intellab-02">
                    <h3 class="has-text-centered">Study B</h3>
                    <video poster="" id="item-intellab-02" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/intellab_02.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-intellab-terrace-00">
                    <h3 class="has-text-centered">Outdoor</h3>
                    <video poster="" id="item-intellab-terrace-00" autoplay controls muted loop playsinline height="100%">
                        <source src="./static/videos/bev/intellab_terrace_00.mp4"
                                type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>
</section>
<!--End BEV snippet carousel -->

<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container">
            <h2 class="title">Poster</h2>

            <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
            </iframe>

        </div>
    </div>
</section> -->
<!--End paper poster -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title has-text-centered">BibTeX</h2>
        <pre><code>
            @inproceedings{blumenkamp2024covisnet,
              title={CoViS-Net: A Cooperative Visual Spatial Foundation Model for Multi-Robot Applications},
              author={Blumenkamp, Jan and Morad, Steven and Gielis, Jennifer and Prorok, Amanda},
              booktitle={Conference on Robot Learning},
              year={2024}
            }
        </code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">

                    <p>
                        This page was built using the <a
                            href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic
                        Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io"
                                                                                target="_blank">Nerfies</a> project
                        page.
                    </p>

                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
